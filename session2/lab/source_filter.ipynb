{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source-filter speech model\n",
    "\n",
    "\n",
    "Outline of the notebook:\n",
    "\n",
    "###  <a href='#1'>1. Introduction</a> \n",
    "\n",
    "Short introduction to speech signals and the source-filter model.\n",
    "\n",
    "###  <a href='#2'>2. Theoretical part</a> \n",
    "\n",
    "**Objective**: Understand the fundamentals of the source-filter speech model based on an autoregressive (AR) modeling (also called linear prediction), and how to estimate its parameters by solving Yule-Walker equations.\n",
    "\n",
    "###  <a href='#3'>3. Practical work</a> \n",
    "\n",
    "* #### <a href='#3.1'> 3.1 Visualization</a> \n",
    "\n",
    "    **Objective**: Visualize a short stationary speech signal (a vowel) in the time and frequency domains. Understand the difference between the pitch and the formants.\n",
    "\n",
    "* #### <a href='#3.2'>3.2 AR model parameters estimation</a> \n",
    "\n",
    "   **Objective**: Fit an AR model on the vowel signal, solving the Yule-Walker equations.\n",
    "\n",
    "* #### <a href='#3.3'>3.3 Spectral envelope</a> \n",
    "\n",
    "    **Objective**: Compute and observe the spectral envelope, which characterizes the resonances in the vocal tract.\n",
    "    \n",
    "* #### <a href='#3.4'>3.4 Residual</a> \n",
    "\n",
    "    **Objective**: Compute and observe the residual of the linear prediction, which corresponds to the speech source signal.\n",
    "\n",
    "* #### <a href='#3.5'>3.5 Synthesis</a> \n",
    "\n",
    "    **Objective**: Synthesize voiced and unvoiced speech signals by linear prediction.\n",
    "    \n",
    "#### <a href='#bonus'>Bonus: Short-term speech analysis/synthesis</a> \n",
    "\n",
    "**Objective**: Extend the previous work to deal with arbitrary-length speech signals.\n",
    "\n",
    "#### <a href='#appendix'>Appendix</a> \n",
    "\n",
    "Technical signal processing details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Generalities\n",
    "\n",
    "In telephony, the  sampling rate of speech signals is chosen to ensure the intelligibility of the message and allow for speaker indentification. These conditions are fullfiled by keeping the frequency band [0 - 4] kHz, known as the voice band. Therefore, we choose a sampling frequency of 8 kHz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Speech sounds\n",
    "\n",
    "We distinguish two types of speech sounds:\n",
    "\n",
    "1. Voiced sounds, with a quasi-periodic waveform. Vowels are a perfect example of voiced sounds. \n",
    "2. Unvoiced sounds, with a noise-like waveform. \n",
    "\n",
    "Vowels are typically longer than consonants. There exist multiple types of consonants: \n",
    "\n",
    "- nasals which are voiced, such as /m/ or /n/;\n",
    "- fricatives which can be voiced in /v/, /z/, /j/ or unvoiced in /f/, /s/, /ch/;\n",
    "- plosives which can be voiced  in /b/, /d/, /g/ or unvoiced in /p/, /t/, /k/;\n",
    "- liquids which are voiced, such as /l/ or /r/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Source-filter speech model\n",
    "\n",
    "<img src=\"./figures/block_diagram.png\" width=\"500px\" align=\"center\">\n",
    "\n",
    "One of the most important concepts for characterizing the structure of speech signals is deduced from the source-filter model proposed by Fant in 1970. In this model the speech signal is seen as a sound source that has undergone spectral modifications according to the shape of the vocal tract, which acts as an acoustic filter. As we have seen, a vocal source can be either voiced, when there is vibration of the vocal folds, or unvoiced, when a turbulent noise is created at a constriction, or following a sudden relaxation of an occlusion in the vocal tract.\n",
    "\n",
    "The two types of speech sounds, voiced and unvoiced, can be modeled as the output of an all-pole (infinite impulse response) linear filter, whose order (number of coefficients) ranges between 10 and 20, and whose input is:\n",
    "\n",
    "- a white noise signal for unvoiced sounds;\n",
    "- a periodic pulse train for voiced sounds.\n",
    "\n",
    "The pulse train associated with voiced sounds corresponds to the sequence of openings and closings of the glottis (the space between the vocal folds), openings being much longer that closings. During the closing phases, the sudden reduction in the air flow causes a brief pulse. The fundamental frequency of this periodic signal is called the pitch. \n",
    "\n",
    "<img src=\"./figures/vocal_fold_signal.png\" width=\"400px\" align=\"center\">\n",
    "\n",
    "<img src=\"./figures/vocal_fold_signal_zoom.png\" width=\"550px\" align=\"center\">\n",
    "\n",
    "For a man, the pitch varies between 90 and 270 Hz, for a woman it varies between 120 and 360 Hz, and for a child between 200 and 600 Hz. For a given speaker, the pitch changes through the conversation, it is part of the prosody. \n",
    "\n",
    "The vocal tract acts as a set of resonators, and gives rise to resonances at specific frequencies which are called the formant frequencies.\n",
    "\n",
    "<img src=\"./figures/formants.png\" width=\"650px\" align=\"center\">\n",
    "\n",
    "Credits for Figures 4.33, 4.34 and 4.38: David Howard, Jamie Angus. Acoustics and Psychoacoustics, Fourth Edition (2009, Focal Press).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Theoretical part\n",
    "\n",
    "### 2.1 Autoregressive model\n",
    "\n",
    "The **source-filter model** can be formalized mathematically with the following **autoregressive model** of order $P$:\n",
    "\n",
    "$$ X(t) + \\sum_{i=1}^P a_i X(t-i) = W(t), \\qquad (1) $$\n",
    "\n",
    "where $W(t)$ is a white Gaussian noise of variance $\\sigma_w^2$ and the polynomial $A(z) = 1 + \\sum_{i=1}^P a_p z^{-p} \\neq 0$ for $|z|>1$.\n",
    "\n",
    "It can be shown that the unique solution to (1) is given by:\n",
    "\n",
    "$$ X(t) = \\sum_{k=0}^{+\\infty} h_k W(t-k) = [W \\star h](t), \\qquad (2)$$\n",
    "\n",
    "where $\\star$ denotes the convolution operator. $X(t)$ corresponds to an **infinite impulse response filtering** of $W(t)$, whose transfer function is given in the Z-transform domain by \n",
    "\n",
    "$$\\displaystyle H(z) = \\frac{1}{A(z)} = \\sum_{k=0}^{+\\infty} h_k z^{-k}.$$ \n",
    "\n",
    "The coefficients $\\{h_k\\}_{k=0}^{+\\infty}$ are the coefficients of the impulse response. This transfer function exhibits resonances at the zeros of the polynomial $A(z)$, which are called the poles of the filter.\n",
    "\n",
    "**For speech modeling, $W(t)$ represents the unvoiced source signal, $H(z)$ the transfer function of the vocal tract filter, and $X(t)$ the resulting speech signal. In practice, we assume that for voiced speech sounds, only the source signal $W(t)$ changes, and it corresponds to a periodic pulse train. However, for the theoretical analysis, we assume that $W(t)$ is a white Gaussian noise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to interpret this AR speech model in terms of **linear prediction**. Equation (1) rewrites as\n",
    "\n",
    "$$ X(t) = \\hat{X}(t) + W(t),$$\n",
    "\n",
    "where $\\hat{X}(t) = - \\sum_{i=1}^P a_i X(t-i)$ is the linear prediction of $X(t)$, computed from the $P$ previous coefficients. \n",
    "\n",
    "LPC10 is a speech coding standard developed in the 70's by the USA government. It is based on this exact AR speech model. At the encoder, we compute P=10 AR coefficients, the energy of the source signal, and the pitch of the voice if the signal is voiced. These parametrs are then sent through the communication channel, and used at the decoder to reconstruct the speech signal by linear prediction, using a white noise or a periodic pulse train source signal.\n",
    "\n",
    "This is what you are going to implement in the rest of this notebook. But before, let's briefly discuss how to estimate the AR model parameters. This is done by solving the so-called Yule-Walker equations, which from the linear prediction perspective is equivalent to minimizing the expected squared error $\\mathbb{E}[(X(t) - \\hat{X}(t))^2].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Estimating the model parameters by solving Yule-Walker equations\n",
    "\n",
    "We are interested in estimating the AR model parameters (the $P$ AR cofficients and the noise variance) from a quasi-stationary portion of speech signal, whose $T$ samples are denoted by $x(0),..., x(T-1)$.\n",
    "\n",
    "Using the properties of the AR random process defined in (1), we can show (the interested reader is referred to the appendix) that the following **Yule-Walker equations** hold:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "\\hat{R}(0) & \\hat{R}(1) & \\cdots & \\hat{R}(P) \\\\\n",
    "\\hat{R}(1) & \\hat{R}(0) & \\cdots & \\hat{R}(P-1) \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "\\hat{R}(P) & \\hat{R}(P-1) & \\cdots & \\hat{R}(0)\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "a_1 \\\\\n",
    "\\vdots \\\\\n",
    "a_P\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\sigma_w^2 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{R}(k)$ is the empirical autocovariance function defined by\n",
    "\n",
    "$$ \\hat{R}(k) = \\frac{1}{T} \\sum_{t=0}^{T-1-k} x(t) x(t+k), \\qquad k \\ge 0. $$\n",
    "\n",
    "For $k < 0$, $\\hat{R}(k) = \\hat{R}(-k)$.\n",
    "\n",
    "We have $P+1$ equations for $P+1$ unknowns, we can therefore solve this system to estimate the model parameters. \n",
    "\n",
    "**In summary, we estimate the model parameters by computing the empirical autocovariance function of the signal and then solving the Yule-Walker equations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Frequency-domain interpretation of the AR speech model\n",
    "\n",
    "The time-domain AR model in (1) leads to the following interpretation in the frequency domain, or more precisely in the domain of the discrete-time Fourier transform (DTFT):\n",
    "\n",
    "$$ \\frac{1}{T} \\Big| \\sum_{t=0}^{T-1} x(t) e^{-j 2 \\pi \\nu t} \\Big|^2 = \\frac{1}{ \\Big| 1 + \\sum_{p=1}^P \\hat{a}_p  e^{-j 2 \\pi \\nu p} \\Big|^2} \\times \\hat{\\sigma}_w ^2, $$\n",
    "\n",
    "where $\\hat{a}_p$ and $\\hat{\\sigma}_w ^2$ are estimates of the AR model parameters, obtained by solving the Yule-Walker equations.\n",
    "\n",
    "The left-hand side is simply the (scaled) **power spectrum of the speech signal**. More precisely, it's an estimate of the power spectral density (PSD) of the random process $X(t)$ that is called the periodogram, but you do not need to remember this.\n",
    "\n",
    "The right-hand side is the product of two terms:\n",
    "\n",
    "- $\\displaystyle \\frac{1}{ \\Big| 1 + \\sum_{p=1}^P \\hat{a}_p  e^{-j 2 \\pi \\nu p} \\Big|^2}$ is called the **spectral envelope**. It corresponds to the frequency response of the all-pole filter in the AR model and it accounts for the **resonances in the vocal tract**. \n",
    "\n",
    "- $\\hat{\\sigma}_w ^2$ is the **power spectrum of the source signal** (actually also an estimate of the source PSD). We say that the source is **white**, because its PSD is constant. This precisely comes from the fact that we assumes $W(t)$ is a white Gaussian noise.\n",
    "\n",
    "**In summary, the spectrum of the speech signal is equal to the multiplication of (i) the flat spectrum of the source signal, and (ii) the frequency response of the all-pole filter which models the resonances (i.e. formants) in the vocal tract.**\n",
    "\n",
    "For more technical details, the interested reader is referred to the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Practical work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile as sf \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load a speech signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all, fs = sf.read('./data/aeiou_8k.wav')\n",
    "\n",
    "print(fs)\n",
    "\n",
    "T_all = x_all.shape[0]\n",
    "time = np.arange(T_all)/fs\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(time, x_all)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude')\n",
    "\n",
    "ipd.Audio(x_all, rate=fs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the toolbox [librosa](https://librosa.github.io/librosa/) to compute the power spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlen_sec=32e-3\n",
    "hop_percent=.5\n",
    "wlen = int(wlen_sec*fs) # window length of 64 ms\n",
    "wlen = int(np.power(2, np.ceil(np.log2(wlen)))) # next power of 2\n",
    "nfft = wlen\n",
    "hop = int(hop_percent*wlen) # hop size\n",
    "win = np.sin(np.arange(.5,wlen-.5+1)/wlen*np.pi); # sine analysis window\n",
    "\n",
    "X = librosa.stft(x_all, n_fft=nfft, hop_length=hop, win_length=wlen, window=win) # STFT\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "librosa.display.specshow(librosa.power_to_db(np.abs(X)**2), sr=fs, hop_length=hop, x_axis='time', y_axis='hz')\n",
    "\n",
    "plt.set_cmap('gray_r')\n",
    "plt.colorbar()\n",
    "plt.clim(vmin=-30)\n",
    "\n",
    "plt.ylabel('frequency (Hz)')\n",
    "plt.xlabel('time (s)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vowels in the previous signal are approximately 0.5-second long, and they start at:\n",
    "\n",
    "- 0.25 second for /a/\n",
    "- 1.2 second for /e/\n",
    "- 2.2 second for /i/\n",
    "- 3.1 second for /o/\n",
    "- 4.1 second for /u/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 - Visualization\n",
    "\n",
    "In the following cell, we\n",
    "\n",
    "- extract a single vowel (i.e. a stationary portion) from the signal ```x_all```. \n",
    "- listen to the isolated vowel.\n",
    "- visualize the waveform and the power spetrum in dB (discarding the redundant part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0vec = [int(0.25*fs), int(1.2*fs), int(2.2*fs), int(3.1*fs), int(4.1*fs)]\n",
    "\n",
    "ind_vowel = 0\n",
    "\n",
    "x = x_all[t0vec[ind_vowel]:t0vec[ind_vowel]+int(0.5*fs)]\n",
    "T = x.shape[0]\n",
    "\n",
    "time = np.arange(T)/fs\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(time, x)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.title('waveform')\n",
    "\n",
    "X = np.fft.fft(x)[:T//2+1]\n",
    "X_spec = np.abs(X)**2\n",
    "X_spec_db = 10*np.log10(X_spec)\n",
    "\n",
    "freq  = np.arange(0, T//2+1)*fs/T\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(freq, X_spec_db)\n",
    "plt.xlabel('frequency (Hz)')\n",
    "plt.ylabel('magnitude (dB)')\n",
    "plt.title('spectrum')\n",
    "\n",
    "ipd.Audio(x, rate=fs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the spectrum, we clearly identify the pitch of the voice (the fundamental frequency, associated with the first peak in the spectrum) and the formants (the frequencies of the resonances). You can change the index of the vowel taken from ```x_all```, the fundamental frequency should remain the same and the formants should change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feel free to load in the variable ```x``` a recording of your voice uttering a vowel.** The signal should have a sampling rate of 8 kHz, you can use [librosa.resample](https://librosa.org/doc/0.9.1/generated/librosa.resample.html?highlight=resample#librosa.resample) to resample a signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "### 3.2 - AR model parameters estimation\n",
    "\n",
    "In the following ```LPC``` function, we solve Yule-Walker equations in order to estimate the parameters of the autoregressive speech model of order P.\n",
    "\n",
    "- [```sp.linalg.toeplitz```](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.linalg.toeplitz.html) computes the covariance matrix, which is Toeplitz\n",
    "- [```np.linalg.inv```](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html) computes the inverse of a matrix\n",
    "- ```@``` multiplies two matrices\n",
    "- [```np.concatenate```](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html) concatenates two numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPC(x, P):\n",
    "    \n",
    "    T = x.shape[0]\n",
    "    \n",
    "    r = np.zeros(P+1)\n",
    "    \n",
    "    for i in np.arange(P+1):\n",
    "        \n",
    "        x1 = x[i:T]\n",
    "        x2 = x[:T-i]\n",
    "        r[i] = 1/T*np.sum(x1*x2)\n",
    "        \n",
    "    R = sp.linalg.toeplitz(r[:P])\n",
    "    a = -np.linalg.inv(R)@r[1:]\n",
    "    sigma2 = np.sum(r * np.concatenate([np.array([1.]), a]))\n",
    "\n",
    "    return a, sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call this function to estimate the parameters of an AR model of order $P=16$ on the previous vowel signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 16\n",
    "a, sigma2 = LPC(x, P)\n",
    "print(a)\n",
    "print(sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.3'></a>\n",
    "\n",
    "### 3.3 - Spectral envelope (Exercise)\n",
    "\n",
    "\n",
    "- Compute and plot the periodogram of the signal defined by\n",
    "\n",
    "$$ \\frac{1}{T} \\Big| \\sum_{t=0}^{T-1} x(t) e^{-j 2 \\pi \\nu t} \\Big|^2 $$\n",
    "\n",
    "- Compute and plot on the same figure the scaled spectral envelope defined by\n",
    "\n",
    "  $$ \\frac{\\hat{\\sigma}_w ^2}{ \\Big| 1 + \\sum_{p=1}^P \\hat{a}_p  e^{-j 2 \\pi \\nu p} \\Big|^2},$$\n",
    "\n",
    "  where $\\hat{a}_p$ and $\\hat{\\sigma}_w ^2$ are the estimates of the AR model parameters. Note that the denominator of the spectral envelop involves the DTFT of the sequence $\\{1, a_1, ..., a_P\\}.$\n",
    "  \n",
    "  In practice, you will compute the DFT of order $F = T$ (i.e. $\\nu = f/F$ with $f\\in \\{0,...,F-1\\}$, using ```np.fft.fft```.\n",
    "  \n",
    "  \n",
    "\n",
    "- What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perio_x_db = ?  # periodogram in dB\n",
    "\n",
    "spec_env_db = ? # spectral envelope in dB\n",
    "\n",
    "freq  = np.arange(0, T//2+1)*fs/T\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(freq, perio_x_db)\n",
    "plt.plot(freq, spec_env_db)\n",
    "plt.xlabel('frequency (Hz)')\n",
    "plt.ylabel('magnitude (dB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.4'></a>\n",
    "\n",
    "### 3.4 - Residual (Exercise)\n",
    "\n",
    "Complete the next cell to compute the residual $e(t) = x(t) - \\hat{x}(t)$ with $\\hat{x}(t) = - \\sum_{i=1}^P a_i x(t-i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = np.zeros(T)\n",
    "x_pred = np.zeros(P)\n",
    "for t in np.arange(T):\n",
    "    residual[t] = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, we visualize the waveform, the spectrum and listen to the residual speech signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(T)/fs\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(time, residual)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_res = np.abs(np.fft.fft(residual)[:T//2+1])**2\n",
    "spec_res_db = 10*np.log10(spec_res)\n",
    "\n",
    "freq  = np.arange(0, T//2+1)*fs/T\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(freq, spec_res_db)\n",
    "plt.xlabel('frequency (Hz)')\n",
    "plt.ylabel('magnitude (dB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(residual, rate=fs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How different is the spectrum of the residual from the spectrum of the original signal?\n",
    "\n",
    "Listen to the residuals obtained from different vowels, what do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.5'></a>\n",
    "### 3.5 - Synthesis (Exercise)\n",
    "\n",
    "Complete the next cell to generate recursively the speech signal according to the AR model:\n",
    "\n",
    " $$ x(t) = \\hat{x}(t) + w(t), \\qquad \\hat{x}(t) = - \\sum_{i=1}^P a_i x(t-i), $$\n",
    " \n",
    "when the source $w(t)$ is either:\n",
    "\n",
    "- the residual signal that you computed before,\n",
    "- a white Gaussian noise of variance $\\sigma_w^2$, \n",
    "- a periodic pulse train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'unvoiced' # 'residual', 'unvoiced', or \"voiced\"\n",
    "f0 = 120\n",
    "T0 = int(1/f0*fs)\n",
    "\n",
    "if source=='voiced':\n",
    "    w = np.zeros(T)\n",
    "    w[0:T:T0] = np.sqrt(sigma2)\n",
    "elif source=='unvoiced':\n",
    "    w = np.sqrt(sigma2)*np.random.randn(T)\n",
    "elif source=='residual':\n",
    "    w = residual  \n",
    "    \n",
    "x_gen = np.zeros(T)\n",
    "\n",
    "for t in np.arange(T):\n",
    "    x_gen[t] = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we visualize the waveform, the spectrum and listen to the synthesized speech signals. \n",
    "\n",
    "Compare what you obtain with different types of source signals (unvoiced, voiced or residual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(T)/fs\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(time, x_gen)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_x_gen = np.abs(np.fft.fft(x_gen)[:T//2+1])**2\n",
    "spec_x_gen_db = 10*np.log10(spec_x_gen)\n",
    "\n",
    "freq  = np.arange(0, T//2+1)*fs/T\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(freq, spec_x_gen_db)\n",
    "plt.xlabel('frequency (Hz)')\n",
    "plt.ylabel('magnitude (dB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(x_gen, rate=fs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bonus'></a>\n",
    "## Bonus: Short-term analysis/synthesis (exercise)\n",
    "\n",
    "In the experiments before, we analysed and synthesized a short stationary speech signal. However, in practice we have to deal with non-stationary speech signals of arbitrary length. Therefore, we have to process the signal on short overlapping frames where we can assume that the signal is stationary, as in the short-time Fourier transform.\n",
    "\n",
    "This is what is (almost) implemented in the rest of the notebook. Complete and run the cells below, and play with the ```voiced``` and ```f0```  variables to make the synthesized speech sound like a whispered voice or a robot's voice. **Feel free to use a recording of your own voice, or any other speech signal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all, fs_x = sf.read('./data/voice_man_1.wav')\n",
    "\n",
    "if fs_x != fs:\n",
    "    # resample to 8 kHz, if necessary\n",
    "    ratio = float(fs) / float(fs_x)\n",
    "    n_samples = int(np.ceil(x_all.shape[-1] * ratio))\n",
    "    x_all = sp.signal.resample(x_all, n_samples, axis=-1)\n",
    "    \n",
    "ipd.Audio(x_all, rate=fs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fs = 8000\n",
    "L = 240 # window length (30 ms @ 8 kHz)\n",
    "H = 120 # hop size\n",
    "P = 16 # order of the AR model\n",
    "\n",
    "voiced = False # boolean indicating if voice should be voiced or not\n",
    "f0 = 50 # fundamental frequency in Hz (if voiced)\n",
    "T0 = int(1/f0*fs) # associated period\n",
    "\n",
    "T = x_all.shape[0]\n",
    "\n",
    "N = int(np.fix( (T-L)/H)) # number of frames\n",
    "\n",
    "win = np.sin(np.arange(.5,L-.5+1)/L*np.pi); # sine analysis window\n",
    "\n",
    "x_all_hat = np.zeros(T) # synthesized signal\n",
    "\n",
    "# Loop over the frames\n",
    "for n in np.arange(N):\n",
    "    \n",
    "    # select a small frame and multiply it by a smooth window, as in the STFT\n",
    "    n1 = n*H\n",
    "    n2 = n1 + L\n",
    "    x = x_all[n1:n2]*win \n",
    "    \n",
    "    # compute the LPC\n",
    "    a, sigma2 = LPC(x, P)\n",
    "    \n",
    "    # synthesize the source signal\n",
    "    if voiced:\n",
    "        w = np.zeros(L)\n",
    "        w[0:T:T0] = np.sqrt(sigma2)*5 \n",
    "    else:\n",
    "        w = np.sqrt(sigma2)*np.random.randn(L)\n",
    "    \n",
    "    # synthesize recursively the speech signal according to the AR model\n",
    "    # this is exactly what you've done in Section 3.5 \"Synthesis\"\n",
    "    x_hat = np.zeros(L)\n",
    "    x_hat_pred =  np.zeros(P)\n",
    "    \n",
    "    for t in np.arange(L):\n",
    "        x_hat[t] = ?\n",
    "\n",
    "    # perform overlap-add, as in the STFT\n",
    "    x_all_hat[n1:n2] += x_hat*win\n",
    "\n",
    "    \n",
    "time = np.arange(T)/fs\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(time, x_all_hat)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude')\n",
    "\n",
    "ipd.Audio(x_all_hat, rate=fs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='appendix'></a>\n",
    "\n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive model\n",
    "\n",
    "We consider the following **autoregressive model** of order $P$:\n",
    "\n",
    "$$ X(t) + \\sum_{i=1}^P a_i X(t-i) = W(t), \\qquad (1) $$\n",
    "\n",
    "where $W(t)$ is a white Gaussian noise of variance $\\sigma_w^2$ and the polynomial $A(z) = 1 + \\sum_{i=1}^P a_p z^{-p} \\neq 0$ for $|z|>1$\n",
    "\n",
    "The unique solution to (1) is given by:\n",
    "\n",
    "$$ X(t) = h_0 W(t) + h_1 W(t-1)\\,\\, +\\,\\, ...\\,\\, = \\sum_{k=0}^{+\\infty} h_k W(t-k) = [W \\star h](t), \\qquad (2)$$\n",
    "\n",
    "where $\\star$ denotes the convolution operator. $X(t)$ corresponds to an **infinite impulse response filtering** of $W(t)$, whose transfer function is given by \n",
    "\n",
    "$$\\displaystyle H(z) = \\frac{1}{A(z)} = \\sum_{k=0}^{+\\infty} h_k z^{-k}.$$ \n",
    "\n",
    "The coefficients $\\{h_k\\}_{k=0}^{+\\infty}$ are the coefficients of the impulse response. This transfer function exhibits resonances at the zeros of the polynomial $A(z)$, which are called the poles of the filter.\n",
    "\n",
    "**For speech modeling, $W(t)$ represents the unvoiced source signal, $H(z)$ the transfer function of the vocal tract filter, and $X(t)$ the resulting speech signal. In practice, we assume that for voiced speech sounds, only the source signal $W(t)$ changes, and it corresponds to a periodic pulse train. However, for the theoretical analysis, we assume that $W(t)$ is a white Gaussian noise.**\n",
    "\n",
    "We can show that $X(t)$ is a wide-sense stationary (WSS) random process, in particular:\n",
    "\n",
    "- The mean $\\mathbb{E}[X(t)] = 0$ does not depend on $t$\n",
    "- $\\mathbb{E}[|X(t)|^2] < \\infty$\n",
    "- The autocovariance function $R(k) = \\mathbb{E}[X(t)X(t+k)]$ only depends on $k$.\n",
    "\n",
    "We recall that because $X(t)$ is WSS, $R(k) = R(-k)$, so in the following we limit the study to $k \\ge 0$. \n",
    "\n",
    "Also, as $W(t)$ is a white Gaussian noise of variance $\\sigma_w^2$, we have:\n",
    "- $\\mathbb{E}[W(t)] = 0$\n",
    "- $\\mathbb{E}[|W(t)|^2] = \\sigma_w^2$\n",
    "- $\\mathbb{E}[W(t)W(t+k)] = 0$ for $k \\neq 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the model parameters by solving Yule-Walker equations\n",
    "\n",
    "In this section we will see how to compute the parameters of this model from an observation of the random process $X(t)$. The model parameters are the autoregressive coefficients $\\{a_1, ..., a_P\\}$ and the noise variance $\\sigma_w^2$, involved in (1).\n",
    "\n",
    "1. Using (2), we can show that (optional exercise)\n",
    "\n",
    "$$\\mathbb{E}[X(t-k)W(t)] = 0, \\qquad \\forall k \\ge 1. \\qquad (3)$$\n",
    "\n",
    "2. Using (1) and (3), we ca show that (optional exercise)\n",
    "\n",
    "$$\\mathbb{E}[X(t)W(t)] = \\sigma_w^2. \\qquad (4)$$\n",
    "\n",
    "3. Using (1), (3) and (4), we can show that the covariance function $R(k) = \\mathbb{E}[X(t)X(t+k)]$ satisfies (optional exercise)\n",
    "\n",
    "  $$R(k) + \\sum_{i=1}^P a_i R(k-i) = 0, \\qquad \\forall k \\ge 1. \\qquad (5)$$\n",
    "  $$R(0) + \\sum_{i=1}^P a_i R(i) = \\sigma_w^2.  \\qquad (6)$$\n",
    "  \n",
    "Combining (5) and (6) we obtain the **Yule-Walker equations**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "R(0) & R(1) & \\cdots & R(P) \\\\\n",
    "R(1) & R(0) & \\cdots & R(P-1) \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "R(P) & R(P-1) & \\cdots & R(0)\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "a_1 \\\\\n",
    "\\vdots \\\\\n",
    "a_P\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "\\sigma_w^2 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0\n",
    "\\end{pmatrix}. \\qquad (7)\n",
    "\\end{equation}\n",
    "\n",
    "(6) gives the first line of this system of equations, and (5) the other ones. In (7), we have $P+1$ equations for $P+1$ unknowns (the $P$ AR cofficients and the noise variance). **We can therefore solve this system to estimate the model parameters**. \n",
    "\n",
    "In practice, we have $T$ data coefficients $x(0),..., x(T-1)$ and we replace the autocovariance function $R(k)$ in (7) by the empirical autocovariance function:\n",
    "\n",
    "$$ \\hat{R}(k) = \\frac{1}{T} \\sum_{t=0}^{T-1-k} x(t) x(t+k), \\qquad k \\ge 0. \\qquad (8) $$\n",
    "\n",
    "For $k < 0$, $\\hat{R}(k) = \\hat{R}(-k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power spectral density\n",
    "\n",
    "The power spectral density (PSD) of the process $X(t)$ is defined by the discrete-time Fourier transform (DTFT) of the autocovariance function:\n",
    "\n",
    "$$ S_X(\\nu) = \\sum_{k=-\\infty}^{+\\infty} R(k) e^{-j 2 \\pi \\nu k}, \\qquad \\nu \\in [0,1[. \\qquad (9) $$\n",
    "\n",
    "The PSD represents the repartition of the energy of the random process in the frequency domain.\n",
    "\n",
    "For the AR model (1), we can show that the PSD is equal to:\n",
    "\n",
    "$$ S_{X}(\\nu) = | H(\\nu) |^2 S_{W}(\\nu), \\qquad \\nu \\in [0,1[, \\qquad (10)$$\n",
    "\n",
    "where \n",
    "\n",
    "- $S_{W}(\\nu) = \\sigma_w ^2$ is the DSP of the source signal. We say that the source is **white**, because its DSP is constant over the frequency.\n",
    "\n",
    "- $\\displaystyle H(\\nu) = \\frac{1}{ 1 + \\sum_{p=1}^P a_p  e^{-j 2 \\pi \\nu p}} $ is the transfer function of an all-pole filter, which accounts for the resonances in the vocal tract. \n",
    "\n",
    "- $| H(\\nu) |^2$ is called the **spectral envelope** of the speech signal.\n",
    "\n",
    "In practice, given $T$ data $x(0),..., x(T-1)$, we estimate the PSD by the **periodogram**, which is defined by:\n",
    "\n",
    "$$ \\hat{S}_X(\\nu) = \\sum_{k=-(T-1)}^{T-1} \\hat{R}(k) e^{-j 2 \\pi \\nu k} = \\frac{1}{T} \\Big| \\sum_{t=0}^{T-1} x(t) e^{-j 2 \\pi \\nu t} \\Big|^2. \\qquad (11)$$\n",
    "\n",
    "**We see that the periodogram is simply the scaled power spectrum of the signal.**\n",
    "\n",
    "Identifying (10) and (11) we have:\n",
    "\n",
    "$$ \\frac{1}{T} \\Big| \\sum_{t=0}^{T-1} x(t) e^{-j 2 \\pi \\nu t} \\Big|^2 = \\frac{\\hat{\\sigma}_w ^2}{ \\Big| 1 + \\sum_{p=1}^P \\hat{a}_p  e^{-j 2 \\pi \\nu p} \\Big|^2}, \\qquad (12) $$\n",
    "\n",
    "where $\\hat{a}_p$ and $\\hat{\\sigma}_w ^2$ are estimates of the AR model parameters, obtained by solving the Yule-Walker equations.\n",
    "\n",
    "**Equation (12) can be interpreted as follows: The spectrum of the speech signal is equal to the multiplication of (i) the flat spectrum of the source signal, and (ii) the frequency response of the all-pole filter which models the resonances (i.e. formants) in the vocal tract.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
