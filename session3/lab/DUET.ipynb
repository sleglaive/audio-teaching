{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degenerate Unmixing Estimation Technique\n",
    "\n",
    "**Objectives**: Understand how spatial diversity in anechoic acoustic recordings can be used to build binary time-frequency masks for source separation. Implement the Degenerate Unmixing Estimation Technique (DUET)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Generation of an anechoic mixture of speech signals\n",
    "\n",
    "Based on what you've done in the previous section, we are now going to generate a stereophonic anechoic mixture of $J=3$ speech source signals, that you will then unmix using DUET algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile as sf \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from utils import plot_recording_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "fs = 16000 # sampling rate in Hz\n",
    "\n",
    "wlen_sec = 32e-3 # STFT window length in seconds\n",
    "hop_percent = .5 # hop size as a percent of the window length\n",
    "wlen = int(wlen_sec*fs) # window length in samples\n",
    "wlen = np.int(np.power(2, np.ceil(np.log2(wlen)))) # next power of 2\n",
    "F = wlen//2+1 # number of non-redundant frequency bins\n",
    "hop = np.int(hop_percent*wlen) # hop size in samples\n",
    "win = np.sin(np.arange(.5,wlen-.5+1)/wlen*np.pi); # sine analysis window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recording configuration\n",
    "\n",
    "q_m1 = np.array([-1e-2, 0]) # 1st microphone cartesian coordinates\n",
    "q_m2 = np.array([1e-2, 0]) # 2nd microphone cartesian coordinates\n",
    "q_s = np.array([[.5, .3],\n",
    "               [0, .5],\n",
    "               [-.5, .3]]).T # source cartesian coordinates, shape (2, 3)\n",
    "\n",
    "plot_recording_config(q_m1, q_m2, q_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITD and ILD computation\n",
    "\n",
    "d1 = np.linalg.norm(q_m1[:,np.newaxis] - q_s, axis=0) # sources-to-1st microphone distance\n",
    "d2 = np.linalg.norm(q_m2[:,np.newaxis] - q_s, axis=0) # sources-to-2nd microphone distance\n",
    "\n",
    "c = 344 # sound velocity in m/s\n",
    "\n",
    "a = d1/d2 # inter-microphone level ratio\n",
    "delta_sec = (d2 - d1)/c # time difference of arrival in seconds\n",
    "delta = delta_sec*fs\n",
    "\n",
    "print('Attenuations: %.2f, %.2f, %.2f' % tuple(a))\n",
    "print('Delays: %.2f, %.2f, %.2f' % tuple(delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a mixture of speech signals based on the recording configuration previously specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_mixture, anechoic_FD_auralization\n",
    "# The function ```create_mixture``` creates a mixture signal given a list of\n",
    "# monophonic source signals, the auralization parameters (attenuation and delays) \n",
    "# and STFT parameters. It uses the ```anechoic_FD_auralization``` function you \n",
    "# worked on in the previous activity.\n",
    "\n",
    "data_path = './data'\n",
    "\n",
    "source_wavefiles = ['voice_woman_1.wav', 'voice_man_1.wav','voice_woman_2.wav']\n",
    "source_wavefiles = [os.path.join(data_path, wavefile) for wavefile in source_wavefiles]\n",
    "\n",
    "J = len(source_wavefiles)\n",
    "\n",
    "x = create_mixture(anechoic_FD_auralization, source_wavefiles, a, delta, wlen, hop, win)\n",
    "T = x.shape[0]\n",
    "\n",
    "time_vec = np.arange(T)/fs\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.plot(time_vec, x)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the resulting mixture signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ipd.Audio([x[:,0], x[:,1]], rate=fs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 DUET - Theory\n",
    "\n",
    "**Acknowledgements**: The following presentation of DUET is largely inspired from [S. Rickard, \"The DUET Blind Source Separation\n",
    "Algorithm\", 2007](https://pdfs.semanticscholar.org/1413/746141f2871e0f45056a7696e019b8f8a100.pdf). \n",
    "\n",
    "---\n",
    "\n",
    "Our objective is now to separate the individual speech source signals from their anechoic stereophonic mixture. This is an under-determined (or degenerate) source sepration problem. As we have more sources than microphones, we have to estimate more variables than equations.\n",
    "\n",
    "We will use DUET, the Degenerate Unmixing Estimation Technique, whose principle can be summarized in one sentence: \n",
    "\n",
    "**It is possible to blindly separate an arbitrary number of sources given\n",
    "just two anechoic mixtures provided the time–frequency representations of the sources do not overlap too much, which is a reasonable assumption for speech signals.**\n",
    "\n",
    "**Notations**: Let $S_{j}(f,n)$ denote the STFT of the $j$-th source signal, with $j \\in \\{1, ... J\\}$. We first consider a monophonic mixture signal: \n",
    "$$X(f,n) = \\sum_{j=1}^J S_{j}(f,n).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Binary masking and W-disjoint orthogonality\n",
    "\n",
    " We assume that the source signals have disjoint time-frequency supports. That is, **at a given time-frequency point $(f,n)$, we cannot have more than one source signal with an STFT coefficient that is is non-zero**. This hypothesis is called \"W-disjoint orthogonality\" and can be formalized by:\n",
    "\n",
    "$$S_{j}(f,n)S_{k}(f,n) = 0, \\qquad \\forall (f,n), \\qquad \\forall j \\neq k.$$ \n",
    "\n",
    "This assumption is the mathematical idealization of the condition that it is likely that every time–frequency point in the mixture with significant energy is dominated by the contribution of one source.\n",
    "\n",
    "W-disjoint orthogonality is crucial to DUET because it allows for the separation of a mixture into its component sources using a **binary mask**:\n",
    "\n",
    "$$ S_{j}(f,n) = M_{j}(f,n) X(f,n),$$\n",
    "\n",
    "where the mask is defined by:\n",
    " \n",
    "\\begin{equation*}\n",
    "M_j(f,n) = \\begin{cases}\n",
    "1 & \\text{if } S_{j}(f,n) \\neq 0 \\\\\n",
    "0 & \\text{otherwise} \\\\\n",
    "\\end{cases}.\n",
    "\\end{equation*}\n",
    " \n",
    "But of course, in practice we do not know the mask as we do not have access to the individual source signals. The question is therefore, how do we determine the masks from the observation of the mixture signal? As we are going to see, **DUET exploits the spatial diversity of the anechoic acoustic recording to build the masks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Anechoic stereophonic mixture\n",
    "\n",
    "We consider an anechoic stereophonic mixture **model** defined by:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix} X_{1}(f, n) \\\\ X_{2}(f, n) \\end{bmatrix} =\n",
    "\\begin{bmatrix} 1 & ... & 1 \\\\ a_1 \\exp\\left({-\\imath 2 \\pi \\frac{f}{L} \\delta_1}\\right) & ... & a_J \\exp\\left({-\\imath 2 \\pi \\frac{f}{L} \\delta_J}\\right) \\end{bmatrix}\n",
    "\\begin{bmatrix} S_{1}(f, n) \\\\ \\vdots \\\\ S_{J}(f, n) \\end{bmatrix}. \\qquad (2)\n",
    "\\end{equation*}\n",
    "\n",
    "We further assume that the sources have **different spatial locations**, so that:\n",
    "\n",
    "$$(a_j \\neq a_k) \\text{ or } (\\delta_j \\neq \\delta_k) \\qquad \\forall j \\neq k.$$\n",
    "\n",
    "Indeed, DUET is based on exploiting the spatial diversity in the mixture to estimate the binary masks for source separation.\n",
    "\n",
    "With the further assumption of W-disjoint orthogonality, **at most one source is active at every time-frequency point**, and the mixture model can be rewritten as follows for all $(f,n)$ and for some source index $j$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix} X_{1}(f,n) \\\\ X_{2}(f,n) \\end{bmatrix} =\n",
    "\\begin{bmatrix} 1 \\\\ a_j  \\exp\\left({-\\imath 2 \\pi \\frac{f}{L} \\delta_j}\\right) \\end{bmatrix} S_{j}(f, n).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 DUET algorithm\n",
    "\n",
    "The main observation that DUET leverages is that **the ratio of the time–frequency representations of the mixtures does not depend on the source components but only on the mixing parameters associated with the active source component**:\n",
    "\n",
    "$$\\frac{X_{2}(f,n)}{X_{1}(f,n)} = a_j\\exp\\left({-\\imath 2 \\pi \\frac{f}{L}\\delta_j}\\right), \\qquad \\forall (f, n) \\in \\Omega_j = \\{(f,n),\\, S_{j}(f,n) \\neq 0\\}.$$\n",
    "\n",
    "The mixing parameters associated with each time-frequency point can be calculated by:\n",
    "\n",
    "$$ \\hat{a}(f,n) = \\left|\\frac{X_{2}(f,n)}{X_{1}(f,n)}\\right|,$$\n",
    "\n",
    "$$ \\hat{\\delta}(f,n) = -\\frac{1}{2 \\pi f/L}\\arg\\left(\\frac{X_{2}(f,n)}{X_{1}(f,n)}\\right), \\qquad f > 0.$$\n",
    "\n",
    "Under the assumption that the microphones are sufficiently close together so that the delay estimate is not incorrect due to phase wrapping, **the local attenuation and delay estimators** $\\big(\\hat{a}(f,n), \\hat{\\delta}(f,n)\\big)$ **can only take values among the actual mixing parameters** $\\{(a_j, \\delta_j)\\}_{j=1}^J$.\n",
    "\n",
    "Therefore, we can build a 2D-histogram of the local attenuation and delay estimators to estimate the actual mixing parameters. **We can then associate each time-frequency point of the mixture signal to a given source, in order to perform source separation.**\n",
    "\n",
    "More precisely, time-frequency masks are built as follows:\n",
    "$$ M_j(f, n) =  \\begin{cases}\n",
    "1 & \\text{if } \\big(\\hat{a}(f,n), \\hat{\\delta}(f,n)\\big) = (a_j,\\delta_j) \\\\\n",
    "0 & \\text{otherwise} \\\\\n",
    "\\end{cases}.$$\n",
    "\n",
    "In practice, because not all of the assumptions are strictly satisfied, the local mixing parameter estimates will not be precisely equal to the mixing parameters, but they will cluster around the true values.\n",
    "\n",
    "In summary, DUET consists in:\n",
    "\n",
    "1. Construct the time–frequency representation of both mixtures.\n",
    "\n",
    "2. Take the ratio of the two mixtures and extract local mixing parameter estimates.\n",
    "\n",
    "3. Combine the set of local mixing parameter estimates into $J$ pairings corresponding to the true mixing parameter pairings.\n",
    "\n",
    "4. Generate one binary mask for each determined mixing parameter pair corresponding to the time–frequency points which yield that particular mixing parameter pair.\n",
    "\n",
    "5. Separate the sources by multiplying each mask with one of the mixtures.\n",
    "\n",
    "6. Return each demixed time–frequency representation to the time domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 DUET - Practice\n",
    "\n",
    "**Acknowledgements**: Some pieces of code are inspired from [The Northwestern University Source Separation Library (nussl)](https://github.com/interactiveaudiolab/nussl).\n",
    "\n",
    "---\n",
    "\n",
    "You will now implemement the DUET algorithm. In the following cell, we first remove the leading and trailing silences in the mixture signal, because we can estimate the mixing parameters only for the portions of the mixture that contain speech.\n",
    "\n",
    "#### 2.1 Local mixing parameters computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = librosa.effects.trim(x.T, top_db=30) # remove leading and trailing silences in the mixture\n",
    "x = x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "In the following cell, you have to:\n",
    "\n",
    "- compute the STFTs $X_1(f,n)$ and $X_2(f,n)$ of the left and right microphone signals, respectively.\n",
    "\n",
    "- compute the local attenuation coefficients in dB:\n",
    "\n",
    "    $$\\hat{a}_{dB}(f,n) = 20 \\log_{10} \\hat{a}(f,n),$$\n",
    "\n",
    "    where \n",
    "\n",
    "    $$ \\hat{a}(f,n) = \\left|\\frac{X_{2}(f,n) + \\epsilon}{X_{1}(f,n) + \\epsilon}\\right|,$$\n",
    "\n",
    "    and $\\epsilon = 10^{-10}$ is used to avoid dividing by zero.\n",
    "\n",
    "- compute the local relative delay coefficients (apply the normalization only for $f > 0$):\n",
    "\n",
    "$$\\hat{\\delta}(f,n) = -\\frac{1}{2 \\pi f/L}\\arg\\left(\\frac{X_{2}(f,n)}{X_{1}(f,n)}\\right).$$ \n",
    "\n",
    "__hint__: Use 'np.angle' to compute the argument of a complex number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_L = librosa.stft(x[:,0], n_fft=wlen, hop_length=hop, win_length=wlen, window=win)\n",
    "X_R = librosa.stft(x[:,1], n_fft=wlen, hop_length=hop, win_length=wlen, window=win)\n",
    "\n",
    "F, N = X_L.shape\n",
    "\n",
    "########## TODO ###########\n",
    "\n",
    "inter_channel_ratio = ?\n",
    "\n",
    "attenuation = ? # relative attenuation between the two channels\n",
    "attenuation_dB = ? # relative attenuation in dB\n",
    "relative_delay = ?  # relative delay\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 2D histogram computation and peak picking\n",
    "\n",
    "In the following cell, we plot a 2D histogram of the local mixing parameters $\\Big\\{ \\big(\\hat{a}_{dB}(f,n), \\hat{\\delta}(f,n)\\big) \\Big\\}_{f,n}$ that you extracted. \n",
    "\n",
    "**Question**: What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins_att = 50 # the number of bins for the attenuation dimension\n",
    "nbins_delay = 50 # the number of bins for the delay dimension\n",
    "bins_array = np.array([nbins_att, nbins_delay])\n",
    "\n",
    "range_array = np.array([[-1, 1], [-1, 1]]) #The leftmost and rightmost edges of the bins along each dimension.\n",
    "# All values outside of this range will be considered outliers and not counted in the histogram.\n",
    "\n",
    "# compute the histogram\n",
    "histogram, atn_bins, delay_bins = np.histogram2d(attenuation_dB.flatten(), relative_delay.flatten(), \n",
    "                                                 bins=bins_array, range=range_array)\n",
    "\n",
    "atn_tile = np.tile(atn_bins[1:], (nbins_att, 1)).T\n",
    "delay_tile = np.tile(delay_bins[1:].T, (nbins_delay, 1))\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.pcolormesh(atn_tile, delay_tile, histogram, shading='auto')\n",
    "plt.xlabel('attenuation', fontsize=16)\n",
    "plt.ylabel('delay', fontsize=16)\n",
    "plt.title('attenuation-delay histogram')\n",
    "plt.axis('tight')\n",
    "plt.colorbar()\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_wireframe(atn_tile, delay_tile, histogram, rstride=2, cstride=2)\n",
    "plt.xlabel('attenuation', fontsize=16)\n",
    "plt.ylabel('delay', fontsize=16)\n",
    "plt.title('attenuation-delay histogram')\n",
    "plt.axis('tight')\n",
    "ax.view_init(20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we extract the peaks of the 2D histogram in order to estimate the mixing parameters $\\{(\\hat{a}_j, \\hat{\\delta}_j)\\}_{j=1}^J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import find_peak_indices\n",
    "\n",
    "peak_indices = find_peak_indices(histogram, J, threshold=0.2, min_dist=[5,5])\n",
    "\n",
    "atn_indices = [x[0] for x in peak_indices]\n",
    "delay_indices = [x[1] for x in peak_indices]\n",
    "\n",
    "atn_peak_dB = atn_bins[atn_indices] # attenuation estimates in dB scale, shape (J,)\n",
    "atn_peak = 10**(atn_peak_dB/20)  # attenuation estimates in linear scale, shape (J,)\n",
    "\n",
    "delay_peak = delay_bins[delay_indices] # relative delay estimates, shape (J,)\n",
    "\n",
    "print('Estimated attenuations: %.2f, %.2f, %.2f' % tuple(atn_peak))\n",
    "print('Estimated delays: %.2f, %.2f, %.2f' % tuple(delay_peak))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Compare the estimated values with the true ones from the definition of the recording configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Binary masking\n",
    "\n",
    "Once the peaks have been identified, our goal is to determine the time–frequency masks which will separate each source from the mixtures. This is achieved by assigning each time–frequency point $(f,n)$ to the source of index $j \\in \\{1,...,J\\}$ whose peak $(\\hat{a}_j, \\hat{\\delta}_j)$ is closest to the local parameter estimates $\\big(\\hat{a}(f,n), \\hat{\\delta}(f,n)\\big)$.\n",
    "\n",
    "More precisely, we build the time-frequency masks as follows:\n",
    "\n",
    "$$ M_j(f, n) =  \\begin{cases}\n",
    "1 & \\text{if } \\mathcal{J}(f,n) = j \\\\\n",
    "0 & \\text{otherwise} \\\\\n",
    "\\end{cases},$$\n",
    "\n",
    "where $\\mathcal{J}(f,n)$ is defined by\n",
    "\n",
    "$$ \\mathcal{J}(f,n) = \\underset{j \\in \\{1,...,J\\}}{\\operatorname{argmin}} \\left| \\hat{a}_j  \\exp\\left({-\\imath 2 \\pi \\frac{f}{L} \\hat{\\delta}_j}\\right) - \\hat{a}(f,n) \\exp\\left({-\\imath 2 \\pi \\frac{f}{L} \\hat{\\delta}(f,n)}\\right) \\right|^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we compute the masks by applying this procedure. In particular, we use ```np.argmin``` to find the source index that minimizes the above cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = np.zeros((F, N, J))\n",
    "\n",
    "f_vec = 2 * np.pi * np.arange(F) / wlen \n",
    "f_mat = f_vec[:, np.newaxis]\n",
    "\n",
    "for f in np.arange(F):\n",
    "    for n in np.arange(N):\n",
    "        \n",
    "        cost = np.abs(atn_peak * np.exp(-1j * f_mat[f] * delay_peak) - \n",
    "                      attenuation[f,n] * np.exp(-1j * f_mat[f] * relative_delay[f,n]) )**2\n",
    "        \n",
    "        j = np.argmin(cost)\n",
    "        masks[f,n,j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the masks\n",
    "for j in np.arange(J):   \n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(masks[:,:,j], origin='lower', aspect='auto', extent=[0, (N-1)*hop/fs, 0, fs/2])\n",
    "    plt.xlabel('time (s)')\n",
    "    plt.ylabel('frequency (Hz)')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Source separation\n",
    "\n",
    "**Exercise:** \n",
    "\n",
    "Apply the binary masks to the mixture (use the left microphone signal only) in order to compute the STFT of the individual source signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_est = np.zeros((F,N,J), dtype='complex')\n",
    "\n",
    "########## TODO ###########\n",
    "\n",
    "for j in np.arange(J):\n",
    "    S_est[:,:,j] = ?\n",
    "    \n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separated waveforms are then recovered by inverse STFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_est = np.zeros((T,J))\n",
    "\n",
    "for j in np.arange(J):\n",
    "    s_est[:, j] = librosa.istft(S_est[:,:,j], hop_length=hop, win_length=wlen, window=win, length=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the separated sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(s_est[:, 0], rate=fs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(s_est[:, 1], rate=fs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(s_est[:, 2], rate=fs) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
