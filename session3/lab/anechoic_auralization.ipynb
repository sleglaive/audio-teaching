{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Anechoic acoustic recording\n",
    "\n",
    "**Objectives**: Understand inter-microphone level and time differences. Simulate an anechoic acoustic recording in the short-time Fourier transform (STFT) domain. \n",
    "\n",
    "### 1 Theory\n",
    "\n",
    "#### 1.1 Time-domain anechoic recording\n",
    "\n",
    "We consider an anechoic stereophonic recording of a source signal $s(t) \\in \\mathbb{R}$, with time support $\\{0,...,T-1\\}$. \n",
    "\n",
    "The signal captured at microphone $i \\in \\{1, 2\\}$ is given by:\n",
    "\n",
    "$$x_i(t) = \\frac{1}{\\sqrt{4 \\pi} d_i} s\\left(t - \\frac{d_i}{c} f_s\\right),$$\n",
    "\n",
    "where $d_i = || \\mathbf{q}_{m_i} - \\mathbf{q}_s ||_2 $ is the euclidean distance between the source and microphone $i$, $c = 344 $ m/s is the sound velocity and $f_s$ si the sampling rate.\n",
    "\n",
    "Without loss of generality, we can \"absorb\" the attenuation and delay parameters at the first microphone into the definition of the source signal. We define the \"new\" source signal \n",
    "\n",
    "$$\\tilde{s}(t) = \\frac{1}{\\sqrt{4 \\pi} d_1} s\\left(t - \\frac{d_1}{c} f_s\\right),$$ \n",
    "\n",
    "such that\n",
    "\n",
    "$$ x_1(t) = \\tilde{s}(t), \\qquad  x_2(t) = a \\tilde{s}\\left(t  - \\delta\\right),$$\n",
    "\n",
    "where \n",
    "\n",
    "- $a = d_1/ d_2$ is the inter-microphone level ratio, also called inter-channel level difference (ILD);\n",
    "\n",
    "- $\\displaystyle \\delta = \\frac{d_2 - d_1}{c} f_s$ is the **time difference of arrival** (TDoA) in samples, also called inter-channel time difference (ITD). \n",
    "\n",
    "**ILD and ITD are two important spatial cues for sound source localization**, which can be generalized to recordings in the presence of reverberation (out of the scope of this course). \n",
    "\n",
    "In the following, we will omit the tilde notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Frequency-domain anechoic recording\n",
    "\n",
    "Let $S(f)$ be the discrete Fourier transform (DFT) of $s(t)$, defined for all $f \\in \\{0,...,F-1\\}$ where $F = T/2$ and $T$ is the time-domain support of the signal (we only keep the positive frequencies). It is well known that a delay in time domain is equivalent to phase shift in the frequency domain. We have the following DFT pair:\n",
    "\n",
    "$$ s(t-\\delta) \\overset{\\text{DFT}}{\\longleftrightarrow} \\exp\\left({-\\imath 2 \\pi \\frac{f \\delta}{T}}\\right) S(f).$$\n",
    "\n",
    "\n",
    "Let $S(f,n)$ be the STFT of $s(t)$, defined for all $(f, n) \\in \\{0,...,F-1\\} \\times \\{0,...,N-1\\}$ where $N$ is the number of time frames and $F = L/2 + 1$ with $L$ the size of the analysis window. In the most general case, the following relation however **does not hold**:\n",
    "\n",
    "\\begin{equation*}\n",
    "s(t-\\delta) \\overset{\\text{STFT}}{\\longleftrightarrow} \\exp\\left({-\\imath 2 \\pi \\frac{f \\delta}{L}}\\right) S(f, n), \\qquad (1)\n",
    "\\label{phase_shift_STFT}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Indeed, if the STFT analysis window is 40 ms long and the time delay is several seconds, it is clear that at a given time frame, the STFT of the delayed and original signals are not simply related by a phase shift. However, for **time delays that are small relative to the STFT window length**, we can assume that **this relation holds**. In the following, we will assume that it is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Phase ambiguity and microphone spacing\n",
    "\n",
    "The complex exponential $\\exp\\left({-\\imath 2 \\pi \\frac{f \\delta}{L}}\\right)$ uniquely specifies the TDoA $\\delta$ under the following condition: \n",
    "\n",
    "$$\\left|2 \\pi \\frac{f \\delta}{L}\\right| < \\pi, \\qquad f\\in \\{0,1,...,F-1\\}.$$ \n",
    "\n",
    "otherwise we have an ambiguity due to phase wrap. The maximum value of the TDoA (in samples) is $|\\delta_m| = (\\ell / c) f_s$ where $\\ell$ is the inter-microphone spacing. The maximum value for the frequency index is $F - 1 = L/2$ with $L$ the size of the STFT analysis window. Therefore, the above condition is satisfied if:\n",
    "\n",
    "$$ \\displaystyle \\ell < \\frac{c}{f_s}.$$\n",
    "\n",
    "For a sampling rate of 16 kHz, the inter-microphone spacing should be lower than 2.15 cm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Practice\n",
    "\n",
    "Based on the principles that we've just seen, we are going to simulate an anechoic stereophonic recording of a speech signal. \n",
    "\n",
    "#### 2.1 Preliminaries\n",
    "\n",
    "After importing some packages, we first load a monophonic (i.e. single-microphone) speech signal, listen to it and plot its waveform and power spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import soundfile as sf \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from utils import plot_recording_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wav file\n",
    "\n",
    "data_path = './data'\n",
    "\n",
    "s, fs = librosa.load(os.path.join(data_path,'voice_man_1.wav'), sr=None)\n",
    "T = s.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot waveform and listen\n",
    "\n",
    "time_vec = np.arange(T)/fs\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.plot(time_vec, s)\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('amplitude')\n",
    "\n",
    "ipd.display(ipd.Audio(s, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the STFT and plot the power spectrogram\n",
    "\n",
    "wlen_sec = 32e-3 # STFT window length in seconds\n",
    "hop_percent = .5 # hop size as a percent of the window length\n",
    "wlen = int(wlen_sec*fs) # window length in samples\n",
    "wlen = np.int(np.power(2, np.ceil(np.log2(wlen)))) # next power of 2\n",
    "F = wlen//2+1 # number of non-redundant frequency bins\n",
    "hop = np.int(hop_percent*wlen) # hop size in samples\n",
    "win = np.sin(np.arange(.5,wlen-.5+1)/wlen*np.pi); # sine analysis window\n",
    "\n",
    "S = librosa.stft(s, n_fft=wlen, hop_length=hop, win_length=wlen, window=win) # STFT of the source signal\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "librosa.display.specshow(librosa.power_to_db(np.abs(S)**2), sr=fs, hop_length=hop, x_axis='time', y_axis='hz')\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "plt.ylabel('frequency (Hz)')\n",
    "plt.xlabel('time (s)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Recording configuration\n",
    "\n",
    "We define the recording configuration, i.e. the source and microphone cartesian coordinates (in the horizontal plan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_m1 = np.array([-5e-2, 0]) # 1st microphone cartesian coordinates\n",
    "q_m2 = np.array([5e-2, 0]) # 2nd microphone cartesian coordinates\n",
    "q_s = np.array([0.5, 0.5]) # source cartesian coordinates\n",
    "\n",
    "plot_recording_config(q_m1, q_m2, q_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Spatial cues computation\n",
    "\n",
    "We compute the ILD and ITD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = np.linalg.norm(q_m1 - q_s) # source-to-1st microphone distance\n",
    "d2 = np.linalg.norm(q_m2 - q_s) # source-to-2nd microphone distance\n",
    "\n",
    "c = 344 # sound velocity in m/s\n",
    "\n",
    "a = d1/d2 # inter-microphone level ratio\n",
    "delta_sec = (d2 - d1)/c # time difference of arrival in seconds\n",
    "delta = delta_sec*fs\n",
    "\n",
    "print(' inter-microphone level ratio: %.2f' % a)\n",
    "print(' time difference of arrival: %.2e' % delta_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Auralization\n",
    "\n",
    "#### Exercise:\n",
    "\n",
    "Using the relation (1), complete the following function to compute the microphone signals in the STFT domain, given:\n",
    "- the source signal, \n",
    "- the inter-microphone level ratio, \n",
    "- the time difference of arrival (assumed to be much shorter than the STFT analysis window),\n",
    "- and the STFT parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anechoic_FD_auralization(s, a=1, delta=0, wlen=512, hop=256, win='hann'):\n",
    "\n",
    "    T = s.shape[0] # signal length\n",
    "    S = librosa.stft(s, n_fft=wlen, hop_length=hop, win_length=wlen, window=win) # STFT of the source signal\n",
    "    F, N = S.shape\n",
    "    \n",
    "    X = np.zeros((F, N, 2), dtype='complex') # STFT of the microphone signals\n",
    "    \n",
    "    ########## TO COMPLETE ###########\n",
    "    \n",
    "    H = ? # should be of shape (F,)\n",
    "    X[:,:,0] = S\n",
    "    X[:,:,1] = S*H[:,np.newaxis] # np.newaxis \"adds\" a dimension\n",
    "    \n",
    "    ##################################\n",
    "    \n",
    "    x = np.zeros((T,2)) \n",
    "    \n",
    "    # iSTFT to get the time-domain microphone signals\n",
    "    x[:,0] = librosa.istft(X[:,:,0], hop_length=hop, win_length=wlen, window=win, length=T)\n",
    "    x[:,1] = librosa.istft(X[:,:,1], hop_length=hop, win_length=wlen, window=win, length=T)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your auralization function with the above speech signal and recording configuration. \n",
    "\n",
    "Listen to the resulting stereophonic signal **with head/earphones** and check that you localize the source as expected (on the right). \n",
    "\n",
    "Change the source position (e.g. make it closer to the left microphone) and verify that it sounds as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = anechoic_FD_auralization(s, a, delta, wlen, hop, win)\n",
    "\n",
    "print(s.shape)\n",
    "print(x.shape)\n",
    "\n",
    "ipd.display(ipd.Audio([x[:,0], x[:,1]], rate=fs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
